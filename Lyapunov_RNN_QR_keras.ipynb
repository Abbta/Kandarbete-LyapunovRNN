{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jzS_qaJhZZBm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "layers = keras.layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for calculating LE"
      ],
      "metadata": {
        "id": "-vkFsAtDZaqg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tT_h6dH6ZZBo"
      },
      "outputs": [],
      "source": [
        "def rnn_jac(Wxh, Whh, ht, xt, phiprime):\n",
        "    \"\"\"\n",
        "    Compute the Jacobian of the RNN with respect to the hidden state ht\n",
        "    :param Wxh: input-to-hidden weight matrix (U)\n",
        "    :param Whh: hidden-to-hidden weight matrix (V)\n",
        "    :param ht: current hidden state\n",
        "    :param xt: current input\n",
        "    :param phiprime: function handle for the derivative of the activation function\n",
        "    :return: Jacobian matrix\n",
        "    \"\"\"\n",
        "    ht = np.reshape(ht, [-1, 1])  # shape: (32, 1)\n",
        "    xt = np.reshape(xt, [-1, 1])  # shape: (32, 1)\n",
        "    # Compute the Jacobian of the RNN with respect to ht\n",
        "\n",
        "\n",
        "    alpha=Wxh@xt + Whh@ht\n",
        "    J=np.diag(phiprime(alpha).flatten())@Whh\n",
        "    return J\n",
        "\n",
        "def calc_LEs(x_batches, h0, RNNlayer, activation_function_prim=lambda x:np.heaviside(x,1), k_LE=1000):\n",
        "    \"\"\"\n",
        "    Calculate the Lyapunov exponents of a batch of sequences using the QR method.\n",
        "    :param x_batches: input sequences (batch_size, T, input_size)\n",
        "    :param h0: initial hidden state (batch_size, hidden_size)\n",
        "    :param RNNlayer: RNN layer object (e.g., tf.keras.layers.SimpleRNN)\n",
        "    :param activation_function_prim: function handle to derivative of activation function used in the RNN layer\n",
        "    :param k_LE: number of Lyapunov exponents to compute\n",
        "    :return: Lyapunov exponents for each batch (batch_size, k_LE)\n",
        "    \"\"\"\n",
        "    #get dimensions\n",
        "    batch_size, hidden_size = h0.shape\n",
        "    batch_sizeX, T, input_size = x_batches.shape\n",
        "    if batch_size != batch_sizeX:\n",
        "        raise ValueError(\"batch size of h and X not compatible\")\n",
        "    L = hidden_size\n",
        "\n",
        "    #get recurrent cell\n",
        "    RNNcell=RNNlayer.cell\n",
        "\n",
        "    # Choose how many exponents to track\n",
        "    k_LE = max(min(L, k_LE), 1)\n",
        "\n",
        "    #save average Lyapunov exponent over the sequence for each batch\n",
        "    lyaps_batches = np.zeros((batch_size, k_LE))\n",
        "    #Loop over input sequence\n",
        "    for batch in range(batch_size):\n",
        "        x=x_batches[batch]\n",
        "        ht=h0[batch]\n",
        "        #Initialize Q\n",
        "        Q = tf.eye(L)\n",
        "        #keep track of average lyapunov exponents\n",
        "        cum_lyaps = tf.zeros((k_LE,))\n",
        "\n",
        "        for t in range(T):\n",
        "            #Get next state ht+1 by taking a reccurent step\n",
        "            xt=x[t]\n",
        "            _, ht = RNNcell(xt, ht)\n",
        "\n",
        "            #Get jacobian J\n",
        "            Wxh, Whh, b = rnn_layer.get_weights()\n",
        "            # Transpose to match math-style dimensions\n",
        "            Wxh = Wxh.T  # Now shape (units, input_dim)\n",
        "            Whh = Whh.T  # Now shape (units, units)\n",
        "            J = rnn_jac(Wxh, Whh, ht, xt, activation_function_prim)\n",
        "            #Get the Lyapunov exponents from qr decomposition\n",
        "            Q=Q@J\n",
        "            Q,R=tf.linalg.qr(Q, full_matrices=False)\n",
        "            cum_lyaps += tf.math.log(tf.math.abs(tf.linalg.diag_part(R[0:k_LE, 0:k_LE])))\n",
        "        lyaps_batches[batch] = cum_lyaps / T\n",
        "    return lyaps_batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code used to test/show implementation"
      ],
      "metadata": {
        "id": "RkJlXIaIZpSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start out with defining and training a toy model"
      ],
      "metadata": {
        "id": "jSKVdKCHdXJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model():\n",
        "    \"\"\"Define and compile a simple RNN model.\"\"\"\n",
        "    z0 = layers.Input(shape=[None, 2])  # time steps unspecified, 2 features\n",
        "    z = layers.SimpleRNN(32, activation=\"tanh\")(z0)\n",
        "    z = layers.Dense(32, activation='relu')(z)\n",
        "    z = layers.Dense(16, activation='relu')(z)\n",
        "    z = layers.Dense(1)(z)\n",
        "\n",
        "    model = keras.models.Model(inputs=z0, outputs=z)\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def train_model(model, X, y, epochs=20, batch_size=10):\n",
        "    \"\"\"Train model with early stopping and LR scheduler.\"\"\"\n",
        "    results = model.fit(\n",
        "        X, y,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        verbose=1,\n",
        "        callbacks=[\n",
        "            keras.callbacks.ReduceLROnPlateau(factor=0.67, patience=3, verbose=1, min_lr=1E-5),\n",
        "            keras.callbacks.EarlyStopping(patience=4, verbose=1)\n",
        "        ]\n",
        "    )\n",
        "    return results\n",
        "\n",
        "# Create some toy data\n",
        "n_samples = 300\n",
        "time_steps = 20\n",
        "X = np.random.rand(n_samples, time_steps, 2)  # [batch, time, features]\n",
        "Y = np.random.rand(n_samples)\n",
        "\n",
        "# Create and train model\n",
        "model = define_model()\n",
        "results = train_model(model, X, Y)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__LaaRB8Zw9s",
        "outputId": "0d7e077c-6db8-4439-d281-e30a4c5aa0e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1614 - val_loss: 0.1215 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1052 - val_loss: 0.1117 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0835 - val_loss: 0.1078 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0769 - val_loss: 0.1391 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0816 - val_loss: 0.1039 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0795 - val_loss: 0.0903 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0920 - val_loss: 0.0967 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0755 - val_loss: 0.1112 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m24/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0715\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0721 - val_loss: 0.1095 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0756 - val_loss: 0.1057 - learning_rate: 6.7000e-04\n",
            "Epoch 10: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can calulate the LEs of the model"
      ],
      "metadata": {
        "id": "2gOrugH_dcKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create some batches of input data and initial hidden states\n",
        "batch_size = 10   # number of sequences\n",
        "T = 20            # length of each sequence\n",
        "input_dim = 2     # size of each x\n",
        "hidden_dim = 32   # size of hidden state\n",
        "\n",
        "X = np.random.rand(batch_size, T, input_dim)\n",
        "H0 = np.random.rand(batch_size, hidden_dim)\n",
        "\n",
        "#Get the rnn layer of the model\n",
        "rnn_layer=model.layers[1]\n",
        "\n",
        "#Define the derivative of the activation function used\n",
        "tanh_prim=lambda x: 1-np.pow(np.tanh(x), 2)\n",
        "\n",
        "#calculate the LEs\n",
        "number_exponents=20\n",
        "LEs=calc_LEs(X,H0, rnn_layer, tanh_prim, number_exponents)\n",
        "#LEs[batch, exponent], exponents are not ordered\n",
        "print(LEs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzVjWE4EdVfY",
        "outputId": "3b71a801-a838-456a-c3a9-b26a12a44de0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.12576437 -0.14891843 -0.15324934 -0.14134982 -0.13428327 -0.12964118\n",
            "  -0.10776679 -0.15571843 -0.11693958 -0.09222397 -0.16342787 -0.12318116\n",
            "  -0.16189055 -0.13907412 -0.18277627 -0.17552233 -0.17539442 -0.17022833\n",
            "  -0.21283035 -0.10185456]\n",
            " [-0.11316749 -0.13745716 -0.14568643 -0.13207491 -0.11823563 -0.11703672\n",
            "  -0.09598998 -0.13540055 -0.11208098 -0.08343199 -0.16282794 -0.11166426\n",
            "  -0.15727566 -0.12789448 -0.16299987 -0.16356137 -0.16367623 -0.16035384\n",
            "  -0.19379027 -0.10163222]\n",
            " [-0.13708967 -0.13387711 -0.13818024 -0.13338847 -0.12198553 -0.13415112\n",
            "  -0.11041255 -0.14069079 -0.12441449 -0.07649969 -0.15082738 -0.13414796\n",
            "  -0.15845719 -0.12535499 -0.16645148 -0.15997513 -0.15692392 -0.17359036\n",
            "  -0.19508681 -0.11404215]\n",
            " [-0.11866204 -0.14052059 -0.13954674 -0.12851325 -0.11859329 -0.11794174\n",
            "  -0.09673239 -0.13649654 -0.10521861 -0.07816082 -0.13795233 -0.11968331\n",
            "  -0.14843284 -0.12963128 -0.16704577 -0.1618019  -0.15577599 -0.15952009\n",
            "  -0.19896838 -0.10463446]\n",
            " [-0.12505618 -0.14515874 -0.15119341 -0.13453856 -0.12571749 -0.13225242\n",
            "  -0.106127   -0.1447506  -0.11286453 -0.08855193 -0.14761144 -0.1210991\n",
            "  -0.15678717 -0.13891196 -0.18187907 -0.17420959 -0.17250367 -0.15829882\n",
            "  -0.20073363 -0.10969357]\n",
            " [-0.114634   -0.12838861 -0.12877871 -0.11916965 -0.12560935 -0.12126946\n",
            "  -0.09468708 -0.13411272 -0.11721347 -0.07767982 -0.14914665 -0.12446742\n",
            "  -0.14683518 -0.1322585  -0.15392733 -0.15520281 -0.14982861 -0.15228999\n",
            "  -0.1798149  -0.10577884]\n",
            " [-0.12586081 -0.14572722 -0.1583391  -0.14682242 -0.12913938 -0.12530987\n",
            "  -0.11179987 -0.16389348 -0.13216102 -0.09790614 -0.17339943 -0.13235642\n",
            "  -0.1668018  -0.13455489 -0.18062088 -0.18398423 -0.1738666  -0.18010949\n",
            "  -0.21141136 -0.11648941]\n",
            " [-0.12751523 -0.14591122 -0.15614159 -0.14781083 -0.1277599  -0.13286674\n",
            "  -0.10555515 -0.16079989 -0.12565607 -0.09288992 -0.15789706 -0.13915078\n",
            "  -0.16612662 -0.14081234 -0.17884986 -0.17511861 -0.17189141 -0.17451595\n",
            "  -0.21191211 -0.10903945]\n",
            " [-0.13162321 -0.13946357 -0.1488919  -0.14281578 -0.1228285  -0.13185196\n",
            "  -0.1055799  -0.13526624 -0.12410812 -0.09192385 -0.16408294 -0.12375311\n",
            "  -0.17084596 -0.1371862  -0.17725161 -0.16601694 -0.17205112 -0.16899602\n",
            "  -0.19612265 -0.11802697]\n",
            " [-0.13492092 -0.15924139 -0.14500856 -0.13243969 -0.14016667 -0.13379988\n",
            "  -0.11271155 -0.14737487 -0.13085771 -0.09034325 -0.15864882 -0.13448212\n",
            "  -0.16005319 -0.14656487 -0.17444602 -0.17576519 -0.16578177 -0.17703186\n",
            "  -0.21912715 -0.13269483]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}