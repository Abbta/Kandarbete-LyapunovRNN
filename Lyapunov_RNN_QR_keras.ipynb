{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jzS_qaJhZZBm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "layers = keras.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vkFsAtDZaqg"
      },
      "source": [
        "# Code for calculating LE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tT_h6dH6ZZBo"
      },
      "outputs": [],
      "source": [
        "def rnn_jac(Wxh, Whh, ht, xt, phiprime):\n",
        "    \"\"\"\n",
        "    Compute the Jacobian of the RNN with respect to the hidden state ht\n",
        "    :param Wxh: input-to-hidden weight matrix (U)\n",
        "    :param Whh: hidden-to-hidden weight matrix (V)\n",
        "    :param ht: current hidden state\n",
        "    :param xt: current input\n",
        "    :param phiprime: function handle for the derivative of the activation function\n",
        "    :return: Jacobian matrix\n",
        "    \"\"\"\n",
        "    ht = np.reshape(ht, [-1, 1])  # shape: (32, 1)\n",
        "    xt = np.reshape(xt, [-1, 1])  # shape: (32, 1)\n",
        "    # Compute the Jacobian of the RNN with respect to ht\n",
        "\n",
        "\n",
        "    alpha=Wxh@xt + Whh@ht\n",
        "    J=np.diag(phiprime(alpha).flatten())@Whh\n",
        "    return J\n",
        "\n",
        "def calc_LEs(x_batches, h0, RNNlayer, activation_function_prim=lambda x:np.heaviside(x,1), k_LE=1000):\n",
        "    \"\"\"\n",
        "    Calculate the Lyapunov exponents of a batch of sequences using the QR method.\n",
        "    :param x_batches: input sequences (batch_size, T, input_size)\n",
        "    :param h0: initial hidden state (batch_size, hidden_size)\n",
        "    :param RNNlayer: RNN layer object (e.g., tf.keras.layers.SimpleRNN)\n",
        "    :param activation_function_prim: function handle to derivative of activation function used in the RNN layer\n",
        "    :param k_LE: number of Lyapunov exponents to compute\n",
        "    :return: Lyapunov exponents for each batch (batch_size, k_LE)\n",
        "    \"\"\"\n",
        "    #get dimensions\n",
        "    batch_size, hidden_size = h0.shape\n",
        "    batch_sizeX, T, input_size = x_batches.shape\n",
        "    if batch_size != batch_sizeX:\n",
        "        raise ValueError(\"batch size of h and X not compatible\")\n",
        "    L = hidden_size\n",
        "\n",
        "    #get recurrent cell\n",
        "    RNNcell=RNNlayer.cell\n",
        "\n",
        "    # Choose how many exponents to track\n",
        "    k_LE = max(min(L, k_LE), 1)\n",
        "\n",
        "    #save average Lyapunov exponent over the sequence for each batch\n",
        "    lyaps_batches = np.zeros((batch_size, k_LE))\n",
        "    #Loop over input sequence\n",
        "    for batch in range(batch_size):\n",
        "        x=x_batches[batch]\n",
        "        ht=h0[batch]\n",
        "        #Initialize Q\n",
        "        Q = tf.eye(L)\n",
        "        #keep track of average lyapunov exponents\n",
        "        cum_lyaps = tf.zeros((k_LE,))\n",
        "\n",
        "        for t in range(T):\n",
        "            #Get next state ht+1 by taking a reccurent step\n",
        "            xt=x[t]\n",
        "            xt = tf.cast(tf.reshape(xt, [1, input_size]), tf.float32); ht = tf.cast(tf.reshape(ht, [1, L]), tf.float32); _, ht = RNNcell(xt, ht)\n",
        "\n",
        "            #Get jacobian J\n",
        "            Wxh, Whh, b = RNNlayer.get_weights()\n",
        "            # Transpose to match math-style dimensions\n",
        "            Wxh = Wxh.T  # Now shape (units, input_dim)\n",
        "            Whh = Whh.T  # Now shape (units, units)\n",
        "            J = rnn_jac(Wxh, Whh, ht, xt, activation_function_prim)\n",
        "            #Get the Lyapunov exponents from qr decomposition\n",
        "            Q=Q@J\n",
        "            Q,R=tf.linalg.qr(Q, full_matrices=False)\n",
        "            cum_lyaps += tf.math.log(tf.math.abs(tf.linalg.diag_part(R[0:k_LE, 0:k_LE])))\n",
        "        lyaps_batches[batch] = cum_lyaps / T\n",
        "    return lyaps_batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkJlXIaIZpSR"
      },
      "source": [
        "# Code used to test/show implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSKVdKCHdXJ6"
      },
      "source": [
        "Start out with defining and training a toy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "__LaaRB8Zw9s",
        "outputId": "0d7e077c-6db8-4439-d281-e30a4c5aa0e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "27/27 [==============================] - 2s 22ms/step - loss: 0.2036 - val_loss: 0.0852 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.0942 - val_loss: 0.0857 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.0937 - val_loss: 0.0863 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.0896 - val_loss: 0.0847 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.0883 - val_loss: 0.0838 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.0874 - val_loss: 0.0824 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.0900 - val_loss: 0.0798 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.0869 - val_loss: 0.0814 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.0879 - val_loss: 0.0835 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.0870 - val_loss: 0.0785 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.0858 - val_loss: 0.0852 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.0858 - val_loss: 0.0835 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "26/27 [===========================>..] - ETA: 0s - loss: 0.0814\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.0838 - val_loss: 0.0794 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.0830 - val_loss: 0.0768 - lr: 6.7000e-04\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.0805 - val_loss: 0.0787 - lr: 6.7000e-04\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.0806 - val_loss: 0.0788 - lr: 6.7000e-04\n",
            "Epoch 17/20\n",
            "26/27 [===========================>..] - ETA: 0s - loss: 0.0770\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.0773 - val_loss: 0.0809 - lr: 6.7000e-04\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.0763 - val_loss: 0.0828 - lr: 4.4890e-04\n",
            "Epoch 18: early stopping\n"
          ]
        }
      ],
      "source": [
        "def define_model():\n",
        "    \"\"\"Define and compile a simple RNN model.\"\"\"\n",
        "    z0 = layers.Input(shape=[None, 2])  # time steps unspecified, 2 features\n",
        "    z = layers.SimpleRNN(32, activation=\"tanh\")(z0)\n",
        "    z = layers.Dense(32, activation='relu')(z)\n",
        "    z = layers.Dense(16, activation='relu')(z)\n",
        "    z = layers.Dense(1)(z)\n",
        "\n",
        "    model = keras.models.Model(inputs=z0, outputs=z)\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def train_model(model, X, y, epochs=20, batch_size=10):\n",
        "    \"\"\"Train model with early stopping and LR scheduler.\"\"\"\n",
        "    results = model.fit(\n",
        "        X, y,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        verbose=1,\n",
        "        callbacks=[\n",
        "            keras.callbacks.ReduceLROnPlateau(factor=0.67, patience=3, verbose=1, min_lr=1E-5),\n",
        "            keras.callbacks.EarlyStopping(patience=4, verbose=1)\n",
        "        ]\n",
        "    )\n",
        "    return results\n",
        "\n",
        "# Create some toy data\n",
        "n_samples = 300\n",
        "time_steps = 20\n",
        "X = np.random.rand(n_samples, time_steps, 2)  # [batch, time, features]\n",
        "Y = np.random.rand(n_samples)\n",
        "\n",
        "# Create and train model\n",
        "model = define_model()\n",
        "results = train_model(model, X, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gOrugH_dcKA"
      },
      "source": [
        "Now we can calulate the LEs of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzVjWE4EdVfY",
        "outputId": "3b71a801-a838-456a-c3a9-b26a12a44de0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.08115374 -0.13219818 -0.18581873 -0.12058721 -0.18589953 -0.13813955\n",
            "  -0.09557717 -0.17561284 -0.18069313 -0.20732288 -0.11960275 -0.14068384\n",
            "  -0.16383703 -0.13621159 -0.20968314 -0.1637935  -0.1433326  -0.15949938\n",
            "  -0.17264342 -0.19374862]\n",
            " [-0.08273657 -0.16165161 -0.2133095  -0.11809427 -0.19154061 -0.16152157\n",
            "  -0.12143986 -0.22078271 -0.20351167 -0.23067811 -0.14552125 -0.18154164\n",
            "  -0.18714103 -0.16781119 -0.21152203 -0.18963881 -0.13863254 -0.16378766\n",
            "  -0.16923454 -0.22941992]\n",
            " [-0.08641966 -0.1543588  -0.23745427 -0.12475763 -0.20036158 -0.17288859\n",
            "  -0.11694592 -0.21285319 -0.21934381 -0.25415507 -0.14834574 -0.18094803\n",
            "  -0.19039956 -0.17788833 -0.21843679 -0.18241246 -0.15866964 -0.17567238\n",
            "  -0.18474451 -0.23269439]\n",
            " [-0.0953928  -0.17563416 -0.24302678 -0.12851161 -0.20398378 -0.18718439\n",
            "  -0.11926951 -0.20912775 -0.22246298 -0.26288754 -0.15892358 -0.19757919\n",
            "  -0.20187104 -0.18665013 -0.21957636 -0.19645441 -0.15737028 -0.17545792\n",
            "  -0.19852373 -0.23250768]\n",
            " [-0.09128758 -0.16105583 -0.23241027 -0.12775622 -0.20061843 -0.17593321\n",
            "  -0.11660691 -0.23637453 -0.21590467 -0.26891193 -0.15149    -0.1960346\n",
            "  -0.19519855 -0.18220243 -0.22707191 -0.19069472 -0.148096   -0.17284362\n",
            "  -0.1814526  -0.24640867]\n",
            " [-0.07549122 -0.1483538  -0.20622393 -0.10676317 -0.18192154 -0.15031506\n",
            "  -0.10852839 -0.19248673 -0.1938612  -0.22074039 -0.12762314 -0.16145806\n",
            "  -0.16299959 -0.15712681 -0.18205002 -0.16699371 -0.12652127 -0.15655789\n",
            "  -0.15682204 -0.21804361]\n",
            " [-0.09592409 -0.15161279 -0.2302856  -0.12994239 -0.20611422 -0.1749419\n",
            "  -0.11919556 -0.22370073 -0.2115508  -0.26879677 -0.15336314 -0.1926225\n",
            "  -0.18644944 -0.178388   -0.22241434 -0.18031213 -0.15178616 -0.17303921\n",
            "  -0.18697965 -0.23675856]\n",
            " [-0.10080594 -0.18335786 -0.25045046 -0.1367932  -0.21862309 -0.17688896\n",
            "  -0.13202496 -0.23510575 -0.22244957 -0.25863558 -0.15296283 -0.19219823\n",
            "  -0.20525059 -0.17760341 -0.22971408 -0.19181027 -0.16694006 -0.18128709\n",
            "  -0.20911689 -0.25232095]\n",
            " [-0.09629889 -0.17466018 -0.25549555 -0.1412196  -0.21582429 -0.18334812\n",
            "  -0.12446483 -0.24209026 -0.22682056 -0.28049898 -0.16001727 -0.21717727\n",
            "  -0.19932999 -0.18952504 -0.2157691  -0.19488901 -0.15823349 -0.17526352\n",
            "  -0.18182115 -0.2575838 ]\n",
            " [-0.07210311 -0.13772079 -0.21447721 -0.11149339 -0.18376592 -0.15410797\n",
            "  -0.10302217 -0.18798858 -0.18749283 -0.21716201 -0.12475161 -0.16358547\n",
            "  -0.1723257  -0.14821537 -0.18288198 -0.15801333 -0.13137466 -0.14808252\n",
            "  -0.15923467 -0.205928  ]]\n"
          ]
        }
      ],
      "source": [
        "#create some batches of input data and initial hidden states\n",
        "batch_size = 10   # number of sequences\n",
        "T = 20            # length of each sequence\n",
        "input_dim = 2     # size of each x\n",
        "hidden_dim = 32   # size of hidden state\n",
        "\n",
        "X = np.random.rand(batch_size, T, input_dim)\n",
        "H0 = np.random.rand(batch_size, hidden_dim)\n",
        "\n",
        "#Get the rnn layer of the model\n",
        "rnn_layer=model.layers[1]\n",
        "\n",
        "#Define the derivative of the activation function used\n",
        "tanh_prim=lambda x: 1-np.power(np.tanh(x), 2)\n",
        "\n",
        "#calculate the LEs\n",
        "number_exponents=20\n",
        "LEs=calc_LEs(X,H0, rnn_layer, tanh_prim, number_exponents)\n",
        "#LEs[batch, exponent], exponents are not ordered\n",
        "print(LEs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
