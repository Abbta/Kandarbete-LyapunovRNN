{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm #For progress bar\n",
    "import tensorflow as tf\n",
    "from tf import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_jac(Wxh, Whh, ht, xt, phiprime):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian of the RNN with respect to the hidden state ht\n",
    "    :param Wxh: input-to-hidden weight matrix (U)\n",
    "    :param Whh: hidden-to-hidden weight matrix (V)\n",
    "    :param ht: current hidden state\n",
    "    :param xt: current input\n",
    "    :param phiprime: function handle for the derivative of the activation function\n",
    "    :return: Jacobian matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the Jacobian of the RNN with respect to ht\n",
    "\n",
    "    alpha=Wxh@xt + Whh@ht\n",
    "    J=np.diag(phiprime(alpha))@Whh\n",
    "    return J\n",
    "\n",
    "def calc_LEs(x_batches, h0, RNNlayer, activation_function_prim=lambda x:np.heaviside(x,1), k_LE=1000):\n",
    "    \"\"\"\n",
    "    Calculate the Lyapunov exponents of a batch of sequences using the QR method.\n",
    "    :param x_batches: input sequences (batch_size, T, input_size)\n",
    "    :param h0: initial hidden state (batch_size, hidden_size)   \n",
    "    :param RNNlayer: RNN layer object (e.g., tf.keras.layers.SimpleRNN)\n",
    "    :param activation_function_prim: function handle to derivative of activation function used in the RNN layer\n",
    "    :param k_LE: number of Lyapunov exponents to compute\n",
    "    :return: Lyapunov exponents for each batch (batch_size, k_LE)\n",
    "    \"\"\"\n",
    "    #get dimensions\n",
    "    hidden_size = h0.shape\n",
    "    batch_size, T, input_size = x_batches.shape\n",
    "    L = hidden_size\n",
    "\n",
    "    #get recurrent cell\n",
    "    RNNcell=RNNlayer.cell\n",
    "        \n",
    "    # Choose how many exponents to track\n",
    "    k_LE = max(min(L, k_LE), 1)\n",
    "\n",
    "    #save average Lyapunov exponent over the sequence for each batch\n",
    "    lyaps_batches = np.zeros((batch_size, k_LE))\n",
    "    #Loop over input sequence\n",
    "    #tqdm creates a progress bar\n",
    "    for batch in tqdm(range(batch_size)):\n",
    "        x=x_batches[batch]\n",
    "        ht=h0\n",
    "        #Initialize Q\n",
    "        Q = tf.eye(L)\n",
    "        #keep track of average lyapunov exponents\n",
    "        cum_lyaps = tf.zeros((k_LE,))\n",
    "\n",
    "        for t in tqdm(range(T)):\n",
    "            #Get next state ht+1 by taking a reccurent step\n",
    "            xt=x[t]\n",
    "            _, ht = RNNcell(xt, ht)\n",
    "\n",
    "            #Get jacobian J\n",
    "            J = rnn_jac(RNNlayer.get_weights(), ht, xt, activation_function_prim)\n",
    "            \n",
    "            #Get the Lyapunov exponents from qr decomposition\n",
    "            Q=Q@J\n",
    "            Q,R=tf.linalg.qr(Q, full_matrices=False)\n",
    "            cum_lyaps += tf.math.log(tf.linalg.diag_part(R[0:k_LE, 0:k_LE]))\n",
    "        lyaps_batches[batch] = cum_lyaps / T\n",
    "    return lyaps_batches\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
