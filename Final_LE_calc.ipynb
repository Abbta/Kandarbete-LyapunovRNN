{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOil+QMReLaEQkAwO85ZHx/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abbta/Kandarbete-LyapunovRNN/blob/main/Final_LE_calc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "FkufkMLCoYPT",
        "outputId": "6dc73824-7a8e-4e9b-ebea-2870f801121f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "train_model() missing 1 required positional argument: 'callback'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8c5f31906f2c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;31m#lyap_cb = LyapunovCallback(test_sets, k, number_exponents)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: train_model() missing 1 required positional argument: 'callback'"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from scipy.integrate import solve_ivp\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "layers = keras.layers\n",
        "\n",
        "\n",
        "# Configurations for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "# Constants\n",
        "g = 9.81\n",
        "L1 = 1.0\n",
        "L2 = 1.0\n",
        "m1 = 1.0\n",
        "m2 = 1.0\n",
        "\n",
        "# initial states for plot\n",
        "y0 = [np.pi/2, 1, np.pi/2, 1]\n",
        "\n",
        "initials = [[np.pi/2, 1, np.pi/2, 1], [np.pi/4, -1, np.pi/4, -1], [-np.pi/2, 0, np.pi/2, 0], [0.1, 2.0, 3*np.pi/4, -1.5], [np.pi, 0.5, -np.pi/2, -0.5], [-np.pi, -2.0, np.pi/3, 1.0]]\n",
        "initials_lyaps = [1.40176, 1.59057, 2.28863, 2.7653, 2.8809, 2.94079]\n",
        "\n",
        "# ---------------------- Signal Definitions ---------------------- #\n",
        "def f_sin(t, mean, sigma):\n",
        "    \"\"\"Sinusoidal signal with Gaussian noise.\"\"\"\n",
        "    input_noise = np.random.normal(loc=mean, scale=sigma, size=len(t))\n",
        "    return np.sin(2 * np.pi * t), input_noise\n",
        "\n",
        "\n",
        "def f_pendulum(t, mean, sigma, init):\n",
        "\n",
        "    \"\"\"Pendulum signal with Gaussian noise.\"\"\"\n",
        "\n",
        "    θ1, θ2 = solve_pendulum(t, init)\n",
        "    input_noise = np.random.normal(mean, sigma, size=len(t))\n",
        "    if angle_choice == 1:\n",
        "        return θ1, input_noise\n",
        "    else:\n",
        "        return θ2, input_noise\n",
        "\n",
        "# ---------------------- Solve Double Pendulum ---------------------- #\n",
        "\n",
        "def double_pendulum_derivs(t, y):\n",
        "    \"\"\"\n",
        "    EOM of the double pendulum.\n",
        "    \"\"\"\n",
        "    θ1, ω1, θ2, ω2 = y\n",
        "    Δ = θ2 - θ1\n",
        "\n",
        "    # denominators\n",
        "    D1 = (m1 + m2) * L1 - m2 * L1 * np.cos(Δ)**2\n",
        "    D2 = (m1 + m2) * L2 - m2 * L2 * np.cos(Δ)**2\n",
        "\n",
        "    # first‐order angles\n",
        "    dθ1 = ω1\n",
        "    dθ2 = ω2\n",
        "\n",
        "    # ω1 numerator\n",
        "    num1 = (\n",
        "        m2 * L1 * ω1**2 * np.sin(Δ) * np.cos(Δ)\n",
        "        + m2 * g * np.sin(θ2) * np.cos(Δ)\n",
        "        + m2 * L2 * ω2**2 * np.sin(Δ)\n",
        "        - (m1 + m2) * g * np.sin(θ1)\n",
        "    )\n",
        "    dω1 = num1 / D1\n",
        "\n",
        "    # ω2 numerator\n",
        "    num2 = (\n",
        "        -m2 * L2 * ω2**2 * np.sin(Δ) * np.cos(Δ)\n",
        "        + (m1 + m2) * (\n",
        "            g * np.sin(θ1) * np.cos(Δ)\n",
        "            - L1 * ω1**2 * np.sin(Δ)\n",
        "            - g * np.sin(θ2)\n",
        "        )\n",
        "    )\n",
        "    dω2 = num2 / D2\n",
        "\n",
        "    return [dθ1, dω1, dθ2, dω2]\n",
        "\n",
        "\n",
        "def solve_pendulum(t, init):\n",
        "    \"\"\"\n",
        "    Solve the simple and double pendulum equations of motion using solve_ivp.\n",
        "    \"\"\"\n",
        "    if system_type == 'simple' and angle_choice == 2:\n",
        "        raise ValueError(\"Simple pendulum has only one angle. Set angle_choice = 1.\")\n",
        "\n",
        "    elif system_type == 'double':\n",
        "        sol = solve_ivp(double_pendulum_derivs, (t[0], t[-1]), init, t_eval=t, method='DOP853')\n",
        "        θ1, ω1, θ2, ω2 = sol.y[0], sol.y[1], sol.y[2], sol.y[3]\n",
        "\n",
        "    elif 'simple' in system_type:\n",
        "\n",
        "        def simple_pendulum(t, y):\n",
        "            θ, ω = y\n",
        "            return [ω, -g / L1 * np.sin(θ)]\n",
        "\n",
        "        if 'chaotic' in system_type:\n",
        "            sol = solve_ivp(simple_pendulum, (t[0], t[-1]), init[:2], t_eval=t, method='DOP853')\n",
        "        else:\n",
        "            sol = solve_ivp(simple_pendulum, (t[0], t[-1]), [i/10 for i in init[:2]], t_eval=t, method='DOP853') # non chaotic initial condition (first two initals)\n",
        "\n",
        "        θ1, ω1 = sol.y[0], sol.y[1]\n",
        "        θ2 = np.zeros_like(θ1)  # dummy since there is only one angle\n",
        "        ω2 = np.zeros_like(ω1)  # dummy\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid system type\")\n",
        "\n",
        "    return θ1, ω1, θ2, ω2\n",
        "\n",
        "\n",
        "# ---------------------- Lyapunov Calculator ---------------------- #\n",
        "def lp_numeric(k, input_noise_norm, output_noise_norm):\n",
        "    \"\"\"Estimate Lyapunov exponent for the network predictions from input/output noise norms. l here is the network layer index.\"\"\"\n",
        "    return np.log(output_noise_norm / input_noise_norm) / k\n",
        "\n",
        "\n",
        "def lp_analytical(initial, t, end):\n",
        "    \"\"\"\n",
        "    Estimate the largest Lyapunov exponent from two nearby initial conditions for the \"exact\" solution.\n",
        "    \"\"\"\n",
        "\n",
        "    pert = np.random.normal(0, 0.01, size=4) # perturbed initial parameters\n",
        "\n",
        "    # Integrate both trajectories\n",
        "    sol1 = solve_ivp(double_pendulum_derivs, (0, end), initial, t_eval=t, method='DOP853')\n",
        "    sol2 = solve_ivp(double_pendulum_derivs, (0, end), initial + pert, t_eval=t, method='DOP853')\n",
        "\n",
        "    delta = np.linalg.norm(sol2.y - sol1.y, axis=0) # Compute the distance between the two trajectories at each time step\n",
        "    delta_initial = np.linalg.norm(pert) # Initial perturbation magnitude\n",
        "    lyap = np.log(delta / delta_initial) # Lyapunov exponent estimate as a function of time\n",
        "\n",
        "    return lyap / t # return computed lyapunov per time step\n",
        "\n",
        "\n",
        "def rnn_jac(Wxh, Whh, ht, xt, phiprime):\n",
        "    \"\"\"\n",
        "    Compute the Jacobian of the RNN with respect to the hidden state ht\n",
        "    :param Wxh: input-to-hidden weight matrix (U)\n",
        "    :param Whh: hidden-to-hidden weight matrix (V)\n",
        "    :param ht: current hidden state\n",
        "    :param xt: current input\n",
        "    :param phiprime: function handle for the derivative of the activation function\n",
        "    :return: Jacobian matrix\n",
        "    \"\"\"\n",
        "    ht = np.reshape(ht, [-1, 1])  # shape: (32, 1)\n",
        "    xt = np.reshape(xt, [-1, 1])  # shape: (32, 1)\n",
        "    # Compute the Jacobian of the RNN with respect to ht\n",
        "\n",
        "\n",
        "    alpha=Wxh@xt + Whh@ht\n",
        "    J=np.diag(phiprime(alpha).flatten())@Whh\n",
        "    return J\n",
        "\n",
        "def calc_LEs(x_batches, h0, RNNlayer, activation_function_prim=lambda x:np.heaviside(x,1), k_LE=1000):\n",
        "    \"\"\"\n",
        "    Calculate the Lyapunov exponents of a batch of sequences using the QR method.\n",
        "    :param x_batches: input sequences (batch_size, T, input_size)\n",
        "    :param h0: initial hidden state (batch_size, hidden_size)\n",
        "    :param RNNlayer: RNN layer object (e.g., tf.keras.layers.SimpleRNN)\n",
        "    :param activation_function_prim: function handle to derivative of activation function used in the RNN layer\n",
        "    :param k_LE: number of Lyapunov exponents to compute\n",
        "    :return: Lyapunov exponents for each batch (batch_size, k_LE)\n",
        "    \"\"\"\n",
        "    #get dimensions\n",
        "    batch_size, hidden_size = h0.shape\n",
        "    batch_sizeX, T, input_size = x_batches.shape\n",
        "    if batch_size != batch_sizeX:\n",
        "        raise ValueError(\"batch size of h and X not compatible\")\n",
        "    L = hidden_size\n",
        "\n",
        "    #get recurrent cell\n",
        "    RNNcell=RNNlayer.cell\n",
        "\n",
        "    # Choose how many exponents to track\n",
        "    k_LE = max(min(L, k_LE), 1)\n",
        "\n",
        "    #save average Lyapunov exponent over the sequence for each batch\n",
        "    lyaps_batches = np.zeros((batch_size, k_LE))\n",
        "    #Loop over input sequence\n",
        "    for batch in range(batch_size):\n",
        "        x=x_batches[batch]\n",
        "        ht=h0[batch]\n",
        "        #Initialize Q\n",
        "        Q = tf.eye(L)\n",
        "        #keep track of average lyapunov exponents\n",
        "        cum_lyaps = tf.zeros((k_LE,))\n",
        "\n",
        "        for t in range(T):\n",
        "            #Get next state ht+1 by taking a reccurent step\n",
        "            xt=x[t]\n",
        "            _, ht = RNNcell(xt, ht)\n",
        "\n",
        "            #Get jacobian J\n",
        "            Wxh, Whh, b = rnn_layer.get_weights()\n",
        "            # Transpose to match math-style dimensions\n",
        "            Wxh = Wxh.T  # Now shape (units, input_dim)\n",
        "            Whh = Whh.T  # Now shape (units, units)\n",
        "            J = rnn_jac(Wxh, Whh, ht, xt, activation_function_prim)\n",
        "            #Get the Lyapunov exponents from qr decomposition\n",
        "            Q=Q@J\n",
        "            Q,R=tf.linalg.qr(Q, full_matrices=False)\n",
        "            cum_lyaps += tf.math.log(tf.math.abs(tf.linalg.diag_part(R[0:k_LE, 0:k_LE])))\n",
        "        lyaps_batches[batch] = cum_lyaps / T\n",
        "    return lyaps_batches\n",
        "\n",
        "\n",
        "# ---------------------- Data Generation ---------------------- #\n",
        "\n",
        "def get_data_full_state(initial):\n",
        "    \"\"\"\n",
        "    Generate training data using unperturbed dynamics, and return a unperturbed and a perturbed input version.\n",
        "    Also returns the total noise vector as one array on the form [N, 2].\n",
        "    \"\"\"\n",
        "\n",
        "    n = N - window_size - 1\n",
        "    t = np.linspace(0, end, N)\n",
        "\n",
        "    θ1, ω1, θ2, ω2 = solve_pendulum(t, initial)\n",
        "    θ = θ1 if angle_choice == 1 else θ2\n",
        "    ω = ω1 if angle_choice == 1 else ω2\n",
        "\n",
        "    # Generate Gaussian noise for angle and angular velocity\n",
        "    noise = np.random.normal(mean, sigma, size=len(t))\n",
        "\n",
        "     # Perturbed versions\n",
        "    θ_pert = θ + noise\n",
        "    ω_pert = ω + noise\n",
        "\n",
        "    # Normalize θ and ω separately using StandardScaler\n",
        "    scaler_theta = StandardScaler()\n",
        "    scaler_omega = StandardScaler()\n",
        "\n",
        "    θ_scaled = scaler_theta.fit_transform(θ.reshape(-1, 1)).flatten()\n",
        "    ω_scaled = scaler_omega.fit_transform(ω.reshape(-1, 1)).flatten()\n",
        "    θ_pert_scaled = scaler_theta.transform(θ_pert.reshape(-1, 1)).flatten()\n",
        "    ω_pert_scaled = scaler_omega.transform(ω_pert.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Stack noise vectors for full input noise (shape: [N, 2])\n",
        "    input_noise = np.stack([noise, noise], axis=1)\n",
        "\n",
        "    # Construct training sequences\n",
        "    X = np.stack([np.stack([θ_scaled[i: i + window_size], ω_scaled[i: i + window_size]], axis=1) for i in range(n)])\n",
        "    X_pert = np.stack([np.stack([θ_pert_scaled[i: i + window_size], ω_pert_scaled[i: i + window_size]], axis=1) for i in range(n)])\n",
        "\n",
        "    # labels [θ, ω]\n",
        "    y = np.stack([np.array([θ_scaled[i + window_size], ω_scaled[i + window_size]]) for i in range(n)])\n",
        "\n",
        "    # Train/test split\n",
        "    split_idx = int(0.90 * n) # 90% for training, 10% for testing for computational efficiency\n",
        "    return X[:split_idx], y[:split_idx], X_pert[:split_idx], X[split_idx:], y[split_idx:], X_pert[split_idx:], θ, ω, θ_pert, ω_pert, input_noise, t\n",
        "\n",
        "\n",
        "def get_test_sets(initials):\n",
        "    \"\"\"\n",
        "    Generate test sets for different initial conditions.\n",
        "    \"\"\"\n",
        "\n",
        "    test_sets = []\n",
        "    for initial in initials:\n",
        "        _, _, _, X_test, y_test, X_pert_test, *_ = get_data_full_state(initial)\n",
        "        test_sets.append((X_test, y_test, X_pert_test))\n",
        "\n",
        "    return test_sets, X_test, y_test, X_pert_test\n",
        "\n",
        "\n",
        "# ---------------------- Model ---------------------- #\n",
        "class LyapunovCallback(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Custom callback to compute Lyapunov exponents after each epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, test_sets, k, number_exponents):\n",
        "        self.test_sets = test_sets  # list of (X_clean, y, X_pert)\n",
        "        self.k = k\n",
        "        self.number_exponents = number_exponents\n",
        "        self.lyap_list_all = [[] for _ in test_sets]\n",
        "        self.lyap_list_jac_all = [[] for _ in test_sets]\n",
        "        self.lyap_list_jac_noise_all = [[] for _ in test_sets]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        for set_idx, (X_clean_seq, _, X_pert_seq) in enumerate(self.test_sets): # loop over test sets\n",
        "            lyaps = []\n",
        "            lyaps_jac = []\n",
        "\n",
        "            for i in range(len(X_clean_seq)):\n",
        "                x_clean = X_clean_seq[i]\n",
        "                x_pert = X_pert_seq[i]\n",
        "\n",
        "                x_noise = x_pert - x_clean # noise in phase space\n",
        "                input_noise_norm = np.linalg.norm(x_noise) / x_clean.shape[0]\n",
        "\n",
        "                θ_clean_pred, ω_clean_pred = predict_k_steps(self.model, x_clean, self.k)\n",
        "                θ_pert_pred, ω_pert_pred = predict_k_steps(self.model, x_pert, self.k)\n",
        "\n",
        "                output_noise = np.stack([θ_pert_pred - θ_clean_pred, ω_pert_pred - ω_clean_pred], axis=1)\n",
        "                output_noise_norm = np.linalg.norm(output_noise) / len(output_noise)\n",
        "\n",
        "                lyap = lp_numeric(self.k, input_noise_norm, output_noise_norm)\n",
        "                lyaps.append(lyap)\n",
        "\n",
        "\n",
        "                # Jacobian method on a few\n",
        "                H0 = np.random.rand(1, hidden_dim)\n",
        "\n",
        "                x_clean_batch = x_clean[np.newaxis, :, :] # add axis\n",
        "                x_clean_batch_noise = x_noise[np.newaxis, :, :]\n",
        "\n",
        "\n",
        "                lyap_jac = max(calc_LEs(x_clean_batch, H0, rnn_layer, tanh_prim, self.number_exponents)[0])\n",
        "                # lyap_jac_noise = max(calc_LEs(x_clean_batch_noise, H0, rnn_layer, tanh_prim, self.number_exponents)[0])\n",
        "\n",
        "                lyaps_jac.append(lyap_jac)\n",
        "\n",
        "            # Store means for this test set\n",
        "            self.lyap_list_all[set_idx].append(np.mean(lyaps))\n",
        "            self.lyap_list_jac_all[set_idx].append(np.mean(lyaps_jac))\n",
        "            # self.lyap_list_jac_noise_all[set_idx].append(np.mean(lyap_jac_noise))\n",
        "\n",
        "            print(f\" | Epoch {epoch+1} | Initial {set_idx+1}: <PLE λ> = {round(np.mean(lyaps), 4)} s⁻¹, <HSLE λ> = {round(np.mean(lyaps_jac), 4)} s⁻¹\")\n",
        "\n",
        "\n",
        "# ---------------------- Model ---------------------- #\n",
        "\n",
        "def define_model():\n",
        "    \"\"\"Model that takes [θ, ω] sequences and predicts next [θ, ω].\"\"\"\n",
        "\n",
        "    z0 = layers.Input(shape=[None, 2])  # time steps unspecified, 2 features\n",
        "    z = layers.SimpleRNN(32, activation=\"tanh\")(z0)\n",
        "    z = layers.Dense(32, activation='relu')(z)\n",
        "    z = layers.Dense(16, activation='relu')(z)\n",
        "    z = layers.Dense(2)(z)\n",
        "\n",
        "    model = keras.models.Model(inputs=z0, outputs=z)\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model():\n",
        "\n",
        "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)  # reduce lr\n",
        "\n",
        "    results = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        verbose=1,\n",
        "        callbacks=[lr_scheduler]\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ---------------------- Prediction ---------------------- #\n",
        "def predict_k_steps(model, input_window, k):\n",
        "    \"\"\"\n",
        "    Rollout k-step prediction given a model and an input window.\n",
        "    Returns predicted theta and omega arrays (length k).\n",
        "    \"\"\"\n",
        "    x = input_window[np.newaxis, :, :]  # shape (1, window_size, 2)\n",
        "    y_pred = np.zeros((k, 2))\n",
        "\n",
        "    for i in range(k):\n",
        "        y_pred[i] = model.predict(x, verbose=0)[0]\n",
        "        x = np.roll(x, -1, axis=1)  # Shift\n",
        "        x[:, -1, :] = y_pred[i]     # Append new prediction\n",
        "\n",
        "    θ_pred, ω_pred = y_pred[:, 0], y_pred[:, 1]\n",
        "    return θ_pred, ω_pred\n",
        "\n",
        "# ---------------------- Plot ---------------------- #\n",
        "\n",
        "\n",
        "def plot_training(results):\n",
        "    \"\"\"Plot training and validation loss.\"\"\"\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(results.history['loss'], label='Train Loss')\n",
        "    plt.plot(results.history['val_loss'], label='Val Loss')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss (log scale)')\n",
        "    plt.title('Training Progress')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_lyapunov_time(t, lp, label):\n",
        "    \"\"\"\n",
        "    Plot Analytical Lyapunov exponent over time.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(t, lp, label=label)\n",
        "    plt.xlabel('Time [s]')\n",
        "    plt.ylabel(r'$\\lambda(t)$')\n",
        "    plt.title('Lyapunov Exponent Over Time')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_lyapunov_comparison_all(lyap_lists, lyap_jac_lists, labels, train_loss, val_loss, ylabel=r'FTLE $\\lambda$ [s⁻¹]'):\n",
        "    \"\"\"\n",
        "    Plot LE and training/validation loss over epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Left axis: Lyapunov exponents\n",
        "    for i in range(len(labels)):\n",
        "        ax1.plot(lyap_lists[i], marker='o', label=f'{labels[i]} (PLE)')\n",
        "        ax1.plot(lyap_jac_lists[i], marker='x', linestyle='--', label=f'{labels[i]} (HSLE)')\n",
        "\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel(ylabel)\n",
        "    ax1.set_title('Lyapunov exponents and loss during training')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Right axis: Training and validation loss\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(train_loss, color='black', linestyle='-', label='Training loss', alpha=0.3)\n",
        "    ax2.plot(val_loss, color='gray', linestyle='-', label='Validation loss', alpha=0.3)\n",
        "    ax2.set_yscale('log')\n",
        "    ax2.set_ylabel('MSE loss (log scale)')\n",
        "\n",
        "    # Combine legends\n",
        "    lines, labels = ax1.get_legend_handles_labels()\n",
        "    if train_loss is not None or val_loss is not None:\n",
        "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "        lines += lines2\n",
        "        labels += labels2\n",
        "    ax1.legend(lines, labels, loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_lyapunov_noise(lyap_jac_clean, lyap_jac_noise, labels, ylabel=r'Lyapunov exponent $\\lambda$ [s⁻¹]'):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        plt.plot(lyap_jac_clean[i], marker='o', label=f'{labels[i]} (clean)')\n",
        "        plt.plot(lyap_jac_noise[i], marker='x', linestyle='--', label=f'{labels[i]} (noise)')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title('HSLE: clean vs noisy inputs')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def scatter_plot_predictions(model, X_test, y_test, title_suffix='test set'):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    θ_true = y_test[:, 0]\n",
        "    ω_true = y_test[:, 1]\n",
        "    θ_pred = y_pred[:, 0]\n",
        "    ω_pred = y_pred[:, 1]\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Theta\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(θ_true, θ_pred, alpha=0.5, label='Predictions')\n",
        "    plt.plot([θ_true.min(), θ_true.max()], [θ_true.min(), θ_true.max()], 'r--', label='Ideal prediction')\n",
        "    plt.xlabel('Normalized true θ [rad]')\n",
        "    plt.ylabel('Normalized predicted θ [rad]')\n",
        "    plt.title(f'Normalized angle prediction (test set)')\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    # Omega\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(ω_true, ω_pred, alpha=0.5, label='Predictions')\n",
        "    plt.plot([ω_true.min(), ω_true.max()], [ω_true.min(), ω_true.max()], 'r--', label='Ideal prediction')\n",
        "    plt.xlabel('Normalized true ω [rad/s]')\n",
        "    plt.ylabel('Normalized predicted ω [rad/s]')\n",
        "    plt.title(f'Normalized angular velocity prediction (test set)')\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_solution_curve():\n",
        "    \"\"\"\n",
        "    Inspection of solution curves for the simple or double pendulum.\n",
        "    \"\"\"\n",
        "    t = np.linspace(0, 10, 1000)\n",
        "\n",
        "    if system_type == 'simple':\n",
        "        θ1, ω1, θ2, ω2 = solve_pendulum(t, [i/10 for i in y0])\n",
        "    else:\n",
        "        θ1, ω1, θ2, ω2 = solve_pendulum(t, y0)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # θ₁ and θ₂\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(t, θ1, label=r'$\\theta_1(t)$')\n",
        "\n",
        "    if system_type == 'double':\n",
        "        plt.plot(t, θ2, label=r'$\\theta_2(t)$', linestyle='--')\n",
        "\n",
        "    plt.ylabel('Angle [rad]')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # ω₁ and ω₂\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(t, ω1, label=r'$\\omega_1(t)$')\n",
        "\n",
        "    if system_type == 'double':\n",
        "        plt.plot(t, ω2, label=r'$\\omega_2(t)$', linestyle='--')\n",
        "\n",
        "    plt.xlabel('Time [s]')\n",
        "    plt.ylabel('Angular velocity [rad/s]')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.suptitle(f'Solutions curve for the {system_type} pendulum')\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_predicted_vs_true_lyaps_dual(initials_lyap, lyap_cb, labels=None):\n",
        "    \"\"\"\n",
        "    Scatter plot: predicted Lyapunov exponents vs true ones for both prediction-based and Jacobian-based methods.\n",
        "\n",
        "    Parameters:\n",
        "    - initials_lyap: list of true Lyapunov exponents (ground truth)\n",
        "    - lyap_cb: LyapunovCallback instance with prediction history\n",
        "    - labels: optional list of labels for annotation\n",
        "    \"\"\"\n",
        "    # Get final values from last epoch\n",
        "    lyap_pred = [lyaps[-1] for lyaps in lyap_cb.lyap_list_all]\n",
        "    lyap_jac = [lyaps[-1] for lyaps in lyap_cb.lyap_list_jac_all]\n",
        "\n",
        "    plt.figure(figsize=(6.5, 6))\n",
        "    plt.scatter(initials_lyap, lyap_pred, color='blue', marker='o', label='PLE')\n",
        "    plt.scatter(initials_lyap, lyap_jac, color='green', marker='x', label='HSLE')\n",
        "\n",
        "    if labels:\n",
        "        for i, label in enumerate(labels):\n",
        "            plt.annotate(label, (initials_lyap[i], lyap_pred[i]), textcoords=\"offset points\", xytext=(5, 5), ha='left', fontsize=9)\n",
        "            plt.annotate(label, (initials_lyap[i], lyap_jac[i]), textcoords=\"offset points\", xytext=(5, -10), ha='left', fontsize=9)\n",
        "\n",
        "    plt.xlabel(\"Physical systems' Lyapunov exponent [s⁻¹]\")\n",
        "    plt.ylabel('Lyapunov exponent [s⁻¹]')\n",
        "    plt.title(\"Lyapunov exponent vs physical systems' Lyapunov exponent\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def calc_lyaps(test_set, test_X_pert, model, k, no_exps):\n",
        "    (X_clean_seq, _, X_pert_seq) = test_set\n",
        "    lyaps = []\n",
        "    lyaps_jac = []\n",
        "\n",
        "    for i in range(len(X_clean_seq)):\n",
        "        x_clean = X_clean_seq[i]\n",
        "        x_pert = X_pert_seq[i]\n",
        "\n",
        "        x_noise = x_pert - x_clean # noise in phase space\n",
        "        input_noise_norm = np.linalg.norm(x_noise) / x_clean.shape[0]\n",
        "\n",
        "        θ_clean_pred, ω_clean_pred = predict_k_steps(model, x_clean, k)\n",
        "        θ_pert_pred, ω_pert_pred = predict_k_steps(model, x_pert, k)\n",
        "\n",
        "        output_noise = np.stack([θ_pert_pred - θ_clean_pred, ω_pert_pred - ω_clean_pred], axis=1)\n",
        "        output_noise_norm = np.linalg.norm(output_noise) / len(output_noise)\n",
        "\n",
        "        lyap = lp_numeric(k, input_noise_norm, output_noise_norm)\n",
        "        lyaps.append(lyap)\n",
        "\n",
        "\n",
        "        # Jacobian method on a few\n",
        "        H0 = np.random.rand(1, hidden_dim)\n",
        "\n",
        "        x_clean_batch = x_clean[np.newaxis, :, :] # add axis\n",
        "        x_clean_batch_noise = x_noise[np.newaxis, :, :]\n",
        "\n",
        "\n",
        "        lyap_jac = max(calc_LEs(x_clean_batch, H0, rnn_layer, tanh_prim, no_exps)[0])\n",
        "        # lyap_jac_noise = max(calc_LEs(x_clean_batch_noise, H0, rnn_layer, tanh_prim, self.number_exponents)[0])\n",
        "\n",
        "        lyaps_jac.append(lyap_jac)\n",
        "\n",
        "    return np.mean(lyaps), np.mean(lyaps_jac)\n",
        "\n",
        "\n",
        "\n",
        "# ===================================== RUN CODE ==========================================\n",
        "\n",
        "# Parameters\n",
        "system_type = 'simple' # System: 'simple', 'simple chaotic' or 'double'\n",
        "angle_choice = 1 # Angle to use: 1 (θ₁) or 2 (θ₂)\n",
        "\n",
        "\n",
        "# -------- Hyperparameters\n",
        "N = 1000 # number of training sequences\n",
        "window_size = 15\n",
        "mean = 0\n",
        "sigma = 1\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "# ================ only for estimating analytical Lyapunov exponent ================\n",
        "end = 50\n",
        "T_max = 50\n",
        "num_points = 5000\n",
        "t = np.linspace(0, T_max, num_points)\n",
        "#============================================================================\n",
        "\n",
        "i0 = 0 # must be 0 for exact lyapunov prediction to make sense. Starting point after the trainig points.\n",
        "k = 10\n",
        "number_exponents = 20 # number of Lyapunov exponents to compute\n",
        "hidden_dim = 32   # size of hidden state\n",
        "\n",
        "\n",
        "# ============================= INSPECTION =====================================\n",
        "\n",
        "#plot_solution_curve()\n",
        "# plot_lyapunov_time(t, lp_network, 'Network FTLE') # estiamte LE of the physical system as a function of time\n",
        "\n",
        "# ================================== TRAINING ==================================\n",
        "\n",
        "\n",
        "X_train_all, y_train_all, X_pert_all = [], [], []\n",
        "for init in initials:\n",
        "    X, y, X_pert, *_ = get_data_full_state(init)\n",
        "    X_train_all.append(X)\n",
        "    y_train_all.append(y)\n",
        "    X_pert_all.append(X_pert)\n",
        "\n",
        "X_train = np.concatenate(X_train_all)\n",
        "y_train = np.concatenate(y_train_all)\n",
        "X_pert_train = np.concatenate(X_pert_all)\n",
        "\n",
        "\n",
        "test_sets, X_test, y_test, X_pert_test = get_test_sets(initials) # use last one for testing --> scatter plot\n",
        "\n",
        "model = define_model()\n",
        "rnn_layer = model.layers[1]\n",
        "tanh_prim = lambda x: 1 - np.tanh(x)**2\n",
        "#lyap_cb = LyapunovCallback(test_sets, k, number_exponents)\n",
        "\n",
        "results = train_model()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PLE, HSLE=calc_lyaps(test_sets[0], X_pert_test, model, k, number_exponents)"
      ],
      "metadata": {
        "id": "Yczl4CtMl53m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}