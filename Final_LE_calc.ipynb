{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abbta/Kandarbete-LyapunovRNN/blob/main/Final_LE_calc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkufkMLCoYPT",
        "outputId": "f3d7854b-5691-4a12-e1ac-542424bc765b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "75/75 [==============================] - 3s 19ms/step - loss: 0.2822 - val_loss: 0.1084 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.0407 - val_loss: 0.0473 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.0244 - val_loss: 0.0352 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.0171 - val_loss: 0.0302 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.0136 - val_loss: 0.0251 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0116 - val_loss: 0.0232 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0104 - val_loss: 0.0212 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0092 - val_loss: 0.0200 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.0083 - val_loss: 0.0185 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0078 - val_loss: 0.0179 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0073 - val_loss: 0.0162 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0069 - val_loss: 0.0161 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0068 - val_loss: 0.0153 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0066 - val_loss: 0.0148 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0063 - val_loss: 0.0146 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0062 - val_loss: 0.0155 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0060 - val_loss: 0.0139 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0061 - val_loss: 0.0140 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0059 - val_loss: 0.0144 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0060 - val_loss: 0.0141 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from scipy.integrate import solve_ivp\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "layers = keras.layers\n",
        "\n",
        "\n",
        "# Configurations for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "# Constants\n",
        "g = 9.81\n",
        "L1 = 1.0\n",
        "L2 = 1.0\n",
        "m1 = 1.0\n",
        "m2 = 1.0\n",
        "\n",
        "# initial states for plot\n",
        "y0 = [np.pi/2, 1, np.pi/2, 1]\n",
        "\n",
        "initials = [[np.pi/2, 1, np.pi/2, 1], [np.pi/4, -1, np.pi/4, -1], [-np.pi/2, 0, np.pi/2, 0], [0.1, 2.0, 3*np.pi/4, -1.5], [np.pi, 0.5, -np.pi/2, -0.5], [-np.pi, -2.0, np.pi/3, 1.0]]\n",
        "initials_lyaps = [1.40176, 1.59057, 2.28863, 2.7653, 2.8809, 2.94079]\n",
        "\n",
        "# ---------------------- Signal Definitions ---------------------- #\n",
        "def f_sin(t, mean, sigma):\n",
        "    \"\"\"Sinusoidal signal with Gaussian noise.\"\"\"\n",
        "    input_noise = np.random.normal(loc=mean, scale=sigma, size=len(t))\n",
        "    return np.sin(2 * np.pi * t), input_noise\n",
        "\n",
        "\n",
        "def f_pendulum(t, mean, sigma, init):\n",
        "\n",
        "    \"\"\"Pendulum signal with Gaussian noise.\"\"\"\n",
        "\n",
        "    θ1, θ2 = solve_pendulum(t, init)\n",
        "    input_noise = np.random.normal(mean, sigma, size=len(t))\n",
        "    if angle_choice == 1:\n",
        "        return θ1, input_noise\n",
        "    else:\n",
        "        return θ2, input_noise\n",
        "\n",
        "# ---------------------- Solve Double Pendulum ---------------------- #\n",
        "\n",
        "def double_pendulum_derivs(t, y):\n",
        "    \"\"\"\n",
        "    EOM of the double pendulum.\n",
        "    \"\"\"\n",
        "    θ1, ω1, θ2, ω2 = y\n",
        "    Δ = θ2 - θ1\n",
        "\n",
        "    # denominators\n",
        "    D1 = (m1 + m2) * L1 - m2 * L1 * np.cos(Δ)**2\n",
        "    D2 = (m1 + m2) * L2 - m2 * L2 * np.cos(Δ)**2\n",
        "\n",
        "    # first‐order angles\n",
        "    dθ1 = ω1\n",
        "    dθ2 = ω2\n",
        "\n",
        "    # ω1 numerator\n",
        "    num1 = (\n",
        "        m2 * L1 * ω1**2 * np.sin(Δ) * np.cos(Δ)\n",
        "        + m2 * g * np.sin(θ2) * np.cos(Δ)\n",
        "        + m2 * L2 * ω2**2 * np.sin(Δ)\n",
        "        - (m1 + m2) * g * np.sin(θ1)\n",
        "    )\n",
        "    dω1 = num1 / D1\n",
        "\n",
        "    # ω2 numerator\n",
        "    num2 = (\n",
        "        -m2 * L2 * ω2**2 * np.sin(Δ) * np.cos(Δ)\n",
        "        + (m1 + m2) * (\n",
        "            g * np.sin(θ1) * np.cos(Δ)\n",
        "            - L1 * ω1**2 * np.sin(Δ)\n",
        "            - g * np.sin(θ2)\n",
        "        )\n",
        "    )\n",
        "    dω2 = num2 / D2\n",
        "\n",
        "    return [dθ1, dω1, dθ2, dω2]\n",
        "\n",
        "\n",
        "def solve_pendulum(t, init):\n",
        "    \"\"\"\n",
        "    Solve the simple and double pendulum equations of motion using solve_ivp.\n",
        "    \"\"\"\n",
        "    if system_type == 'simple' and angle_choice == 2:\n",
        "        raise ValueError(\"Simple pendulum has only one angle. Set angle_choice = 1.\")\n",
        "\n",
        "    elif system_type == 'double':\n",
        "        sol = solve_ivp(double_pendulum_derivs, (t[0], t[-1]), init, t_eval=t, method='DOP853')\n",
        "        θ1, ω1, θ2, ω2 = sol.y[0], sol.y[1], sol.y[2], sol.y[3]\n",
        "\n",
        "    elif 'simple' in system_type:\n",
        "\n",
        "        def simple_pendulum(t, y):\n",
        "            θ, ω = y\n",
        "            return [ω, -g / L1 * np.sin(θ)]\n",
        "\n",
        "        if 'chaotic' in system_type:\n",
        "            sol = solve_ivp(simple_pendulum, (t[0], t[-1]), init[:2], t_eval=t, method='DOP853')\n",
        "        else:\n",
        "            sol = solve_ivp(simple_pendulum, (t[0], t[-1]), [i/10 for i in init[:2]], t_eval=t, method='DOP853') # non chaotic initial condition (first two initals)\n",
        "\n",
        "        θ1, ω1 = sol.y[0], sol.y[1]\n",
        "        θ2 = np.zeros_like(θ1)  # dummy since there is only one angle\n",
        "        ω2 = np.zeros_like(ω1)  # dummy\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid system type\")\n",
        "\n",
        "    return θ1, ω1, θ2, ω2\n",
        "\n",
        "\n",
        "# ---------------------- Lyapunov Calculator ---------------------- #\n",
        "def lp_numeric(k, input_noise_norm, output_noise_norm):\n",
        "    \"\"\"Estimate Lyapunov exponent for the network predictions from input/output noise norms. l here is the network layer index.\"\"\"\n",
        "    return np.log(output_noise_norm / input_noise_norm) / k\n",
        "\n",
        "\n",
        "def lp_analytical(initial, t, end):\n",
        "    \"\"\"\n",
        "    Estimate the largest Lyapunov exponent from two nearby initial conditions for the \"exact\" solution.\n",
        "    \"\"\"\n",
        "\n",
        "    pert = np.random.normal(0, 0.01, size=4) # perturbed initial parameters\n",
        "\n",
        "    # Integrate both trajectories\n",
        "    sol1 = solve_ivp(double_pendulum_derivs, (0, end), initial, t_eval=t, method='DOP853')\n",
        "    sol2 = solve_ivp(double_pendulum_derivs, (0, end), initial + pert, t_eval=t, method='DOP853')\n",
        "\n",
        "    delta = np.linalg.norm(sol2.y - sol1.y, axis=0) # Compute the distance between the two trajectories at each time step\n",
        "    delta_initial = np.linalg.norm(pert) # Initial perturbation magnitude\n",
        "    lyap = np.log(delta / delta_initial) # Lyapunov exponent estimate as a function of time\n",
        "\n",
        "    return lyap / t # return computed lyapunov per time step\n",
        "\n",
        "\n",
        "def rnn_jac(Wxh, Whh, ht, xt, phiprime):\n",
        "    \"\"\"\n",
        "    Compute the Jacobian of the RNN with respect to the hidden state ht\n",
        "    :param Wxh: input-to-hidden weight matrix (U)\n",
        "    :param Whh: hidden-to-hidden weight matrix (V)\n",
        "    :param ht: current hidden state\n",
        "    :param xt: current input\n",
        "    :param phiprime: function handle for the derivative of the activation function\n",
        "    :return: Jacobian matrix\n",
        "    \"\"\"\n",
        "    ht = np.reshape(ht, [-1, 1])  # shape: (32, 1)\n",
        "    xt = np.reshape(xt, [-1, 1])  # shape: (32, 1)\n",
        "    # Compute the Jacobian of the RNN with respect to ht\n",
        "\n",
        "\n",
        "    alpha=Wxh@xt + Whh@ht\n",
        "    J=np.diag(phiprime(alpha).flatten())@Whh\n",
        "    return J\n",
        "\n",
        "def calc_LEs(x_batches, h0, RNNlayer, activation_function_prim=lambda x:np.heaviside(x,1), k_LE=1000):\n",
        "    \"\"\"\n",
        "    Calculate the Lyapunov exponents of a batch of sequences using the QR method.\n",
        "    :param x_batches: input sequences (batch_size, T, input_size)\n",
        "    :param h0: initial hidden state (batch_size, hidden_size)\n",
        "    :param RNNlayer: RNN layer object (e.g., tf.keras.layers.SimpleRNN)\n",
        "    :param activation_function_prim: function handle to derivative of activation function used in the RNN layer\n",
        "    :param k_LE: number of Lyapunov exponents to compute\n",
        "    :return: Lyapunov exponents for each batch (batch_size, k_LE)\n",
        "    \"\"\"\n",
        "    #get dimensions\n",
        "    batch_size, hidden_size = h0.shape\n",
        "    batch_sizeX, T, input_size = x_batches.shape\n",
        "    if batch_size != batch_sizeX:\n",
        "        raise ValueError(\"batch size of h and X not compatible\")\n",
        "    L = hidden_size\n",
        "\n",
        "    #get recurrent cell\n",
        "    RNNcell=RNNlayer.cell\n",
        "\n",
        "    # Choose how many exponents to track\n",
        "    k_LE = max(min(L, k_LE), 1)\n",
        "\n",
        "    #save average Lyapunov exponent over the sequence for each batch\n",
        "    lyaps_batches = np.zeros((batch_size, k_LE))\n",
        "    #Loop over input sequence\n",
        "    for batch in range(batch_size):\n",
        "        x=x_batches[batch]\n",
        "        ht=h0[batch]\n",
        "        #Initialize Q\n",
        "        Q = tf.eye(L)\n",
        "        #keep track of average lyapunov exponents\n",
        "        cum_lyaps = tf.zeros((k_LE,))\n",
        "\n",
        "        for t in range(T):\n",
        "            #Get next state ht+1 by taking a reccurent step\n",
        "            xt=x[t]\n",
        "            _, ht = RNNcell(xt, ht)\n",
        "\n",
        "            #Get jacobian J\n",
        "            Wxh, Whh, b = rnn_layer.get_weights()\n",
        "            # Transpose to match math-style dimensions\n",
        "            Wxh = Wxh.T  # Now shape (units, input_dim)\n",
        "            Whh = Whh.T  # Now shape (units, units)\n",
        "            J = rnn_jac(Wxh, Whh, ht, xt, activation_function_prim)\n",
        "            #Get the Lyapunov exponents from qr decomposition\n",
        "            Q=Q@J\n",
        "            Q,R=tf.linalg.qr(Q, full_matrices=False)\n",
        "            cum_lyaps += tf.math.log(tf.math.abs(tf.linalg.diag_part(R[0:k_LE, 0:k_LE])))\n",
        "        lyaps_batches[batch] = cum_lyaps / T\n",
        "    return lyaps_batches\n",
        "\n",
        "\n",
        "# ---------------------- Data Generation ---------------------- #\n",
        "\n",
        "def get_data_full_state(initial):\n",
        "    \"\"\"\n",
        "    Generate training data using unperturbed dynamics, and return a unperturbed and a perturbed input version.\n",
        "    Also returns the total noise vector as one array on the form [N, 2].\n",
        "    \"\"\"\n",
        "\n",
        "    n = N - window_size - 1\n",
        "    t = np.linspace(0, end, N)\n",
        "\n",
        "    θ1, ω1, θ2, ω2 = solve_pendulum(t, initial)\n",
        "    θ = θ1 if angle_choice == 1 else θ2\n",
        "    ω = ω1 if angle_choice == 1 else ω2\n",
        "\n",
        "    # Generate Gaussian noise for angle and angular velocity\n",
        "    noise = np.random.normal(mean, sigma, size=len(t))\n",
        "\n",
        "     # Perturbed versions\n",
        "    θ_pert = θ + noise\n",
        "    ω_pert = ω + noise\n",
        "\n",
        "    # Normalize θ and ω separately using StandardScaler\n",
        "    scaler_theta = StandardScaler()\n",
        "    scaler_omega = StandardScaler()\n",
        "\n",
        "    θ_scaled = scaler_theta.fit_transform(θ.reshape(-1, 1)).flatten()\n",
        "    ω_scaled = scaler_omega.fit_transform(ω.reshape(-1, 1)).flatten()\n",
        "    θ_pert_scaled = scaler_theta.transform(θ_pert.reshape(-1, 1)).flatten()\n",
        "    ω_pert_scaled = scaler_omega.transform(ω_pert.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Stack noise vectors for full input noise (shape: [N, 2])\n",
        "    input_noise = np.stack([noise, noise], axis=1)\n",
        "\n",
        "    # Construct training sequences\n",
        "    X = np.stack([np.stack([θ_scaled[i: i + window_size], ω_scaled[i: i + window_size]], axis=1) for i in range(n)])\n",
        "    X_pert = np.stack([np.stack([θ_pert_scaled[i: i + window_size], ω_pert_scaled[i: i + window_size]], axis=1) for i in range(n)])\n",
        "\n",
        "    # labels [θ, ω]\n",
        "    y = np.stack([np.array([θ_scaled[i + window_size], ω_scaled[i + window_size]]) for i in range(n)])\n",
        "\n",
        "    # Train/test split\n",
        "    split_idx = int(0.90 * n) # 90% for training, 10% for testing for computational efficiency\n",
        "    return X[:split_idx], y[:split_idx], X_pert[:split_idx], X[split_idx:], y[split_idx:], X_pert[split_idx:], θ, ω, θ_pert, ω_pert, input_noise, t\n",
        "\n",
        "\n",
        "def get_test_sets(initials):\n",
        "    \"\"\"\n",
        "    Generate test sets for different initial conditions.\n",
        "    \"\"\"\n",
        "\n",
        "    test_sets = []\n",
        "    for initial in initials:\n",
        "        _, _, _, X_test, y_test, X_pert_test, *_ = get_data_full_state(initial)\n",
        "        test_sets.append((X_test, y_test, X_pert_test))\n",
        "\n",
        "    return test_sets, X_test, y_test, X_pert_test\n",
        "\n",
        "\n",
        "# ---------------------- Model ---------------------- #\n",
        "class LyapunovCallback(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Custom callback to compute Lyapunov exponents after each epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, test_sets, k, number_exponents):\n",
        "        self.test_sets = test_sets  # list of (X_clean, y, X_pert)\n",
        "        self.k = k\n",
        "        self.number_exponents = number_exponents\n",
        "        self.lyap_list_all = [[] for _ in test_sets]\n",
        "        self.lyap_list_jac_all = [[] for _ in test_sets]\n",
        "        self.lyap_list_jac_noise_all = [[] for _ in test_sets]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        for set_idx, (X_clean_seq, _, X_pert_seq) in enumerate(self.test_sets): # loop over test sets\n",
        "            lyaps = []\n",
        "            lyaps_jac = []\n",
        "\n",
        "            for i in range(len(X_clean_seq)):\n",
        "                x_clean = X_clean_seq[i]\n",
        "                x_pert = X_pert_seq[i]\n",
        "\n",
        "                x_noise = x_pert - x_clean # noise in phase space\n",
        "                input_noise_norm = np.linalg.norm(x_noise) / x_clean.shape[0]\n",
        "\n",
        "                θ_clean_pred, ω_clean_pred = predict_k_steps(self.model, x_clean, self.k)\n",
        "                θ_pert_pred, ω_pert_pred = predict_k_steps(self.model, x_pert, self.k)\n",
        "\n",
        "                output_noise = np.stack([θ_pert_pred - θ_clean_pred, ω_pert_pred - ω_clean_pred], axis=1)\n",
        "                output_noise_norm = np.linalg.norm(output_noise) / len(output_noise)\n",
        "\n",
        "                lyap = lp_numeric(self.k, input_noise_norm, output_noise_norm)\n",
        "                lyaps.append(lyap)\n",
        "\n",
        "\n",
        "                # Jacobian method on a few\n",
        "                H0 = np.random.rand(1, hidden_dim)\n",
        "\n",
        "                x_clean_batch = x_clean[np.newaxis, :, :] # add axis\n",
        "                x_clean_batch_noise = x_noise[np.newaxis, :, :]\n",
        "\n",
        "\n",
        "                lyap_jac = max(calc_LEs(x_clean_batch, H0, rnn_layer, tanh_prim, self.number_exponents)[0])\n",
        "                # lyap_jac_noise = max(calc_LEs(x_clean_batch_noise, H0, rnn_layer, tanh_prim, self.number_exponents)[0])\n",
        "\n",
        "                lyaps_jac.append(lyap_jac)\n",
        "\n",
        "            # Store means for this test set\n",
        "            self.lyap_list_all[set_idx].append(np.mean(lyaps))\n",
        "            self.lyap_list_jac_all[set_idx].append(np.mean(lyaps_jac))\n",
        "            # self.lyap_list_jac_noise_all[set_idx].append(np.mean(lyap_jac_noise))\n",
        "\n",
        "            print(f\" | Epoch {epoch+1} | Initial {set_idx+1}: <PLE λ> = {round(np.mean(lyaps), 4)} s⁻¹, <HSLE λ> = {round(np.mean(lyaps_jac), 4)} s⁻¹\")\n",
        "\n",
        "\n",
        "# ---------------------- Model ---------------------- #\n",
        "\n",
        "def define_model():\n",
        "    \"\"\"Model that takes [θ, ω] sequences and predicts next [θ, ω].\"\"\"\n",
        "\n",
        "    z0 = layers.Input(shape=[None, 2])  # time steps unspecified, 2 features\n",
        "    z = layers.SimpleRNN(32, activation=\"tanh\")(z0)\n",
        "    z = layers.Dense(32, activation='relu')(z)\n",
        "    z = layers.Dense(16, activation='relu')(z)\n",
        "    z = layers.Dense(2)(z)\n",
        "\n",
        "    model = keras.models.Model(inputs=z0, outputs=z)\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model():\n",
        "\n",
        "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)  # reduce lr\n",
        "\n",
        "    results = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        verbose=1,\n",
        "        callbacks=[lr_scheduler]\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ---------------------- Prediction ---------------------- #\n",
        "def predict_k_steps(model, input_window, k):\n",
        "    \"\"\"\n",
        "    Rollout k-step prediction given a model and an input window.\n",
        "    Returns predicted theta and omega arrays (length k).\n",
        "    \"\"\"\n",
        "    x = input_window[np.newaxis, :, :]  # shape (1, window_size, 2)\n",
        "    y_pred = np.zeros((k, 2))\n",
        "\n",
        "    for i in range(k):\n",
        "        y_pred[i] = model.predict(x, verbose=0)[0]\n",
        "        x = np.roll(x, -1, axis=1)  # Shift\n",
        "        x[:, -1, :] = y_pred[i]     # Append new prediction\n",
        "\n",
        "    θ_pred, ω_pred = y_pred[:, 0], y_pred[:, 1]\n",
        "    return θ_pred, ω_pred\n",
        "\n",
        "# ---------------------- Plot ---------------------- #\n",
        "\n",
        "\n",
        "def plot_training(results):\n",
        "    \"\"\"Plot training and validation loss.\"\"\"\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(results.history['loss'], label='Train Loss')\n",
        "    plt.plot(results.history['val_loss'], label='Val Loss')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss (log scale)')\n",
        "    plt.title('Training Progress')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_lyapunov_time(t, lp, label):\n",
        "    \"\"\"\n",
        "    Plot Analytical Lyapunov exponent over time.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(t, lp, label=label)\n",
        "    plt.xlabel('Time [s]')\n",
        "    plt.ylabel(r'$\\lambda(t)$')\n",
        "    plt.title('Lyapunov Exponent Over Time')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_lyapunov_comparison_all(lyap_lists, lyap_jac_lists, labels, train_loss, val_loss, ylabel=r'FTLE $\\lambda$ [s⁻¹]'):\n",
        "    \"\"\"\n",
        "    Plot LE and training/validation loss over epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Left axis: Lyapunov exponents\n",
        "    for i in range(len(labels)):\n",
        "        ax1.plot(lyap_lists[i], marker='o', label=f'{labels[i]} (PLE)')\n",
        "        ax1.plot(lyap_jac_lists[i], marker='x', linestyle='--', label=f'{labels[i]} (HSLE)')\n",
        "\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel(ylabel)\n",
        "    ax1.set_title('Lyapunov exponents and loss during training')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Right axis: Training and validation loss\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(train_loss, color='black', linestyle='-', label='Training loss', alpha=0.3)\n",
        "    ax2.plot(val_loss, color='gray', linestyle='-', label='Validation loss', alpha=0.3)\n",
        "    ax2.set_yscale('log')\n",
        "    ax2.set_ylabel('MSE loss (log scale)')\n",
        "\n",
        "    # Combine legends\n",
        "    lines, labels = ax1.get_legend_handles_labels()\n",
        "    if train_loss is not None or val_loss is not None:\n",
        "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "        lines += lines2\n",
        "        labels += labels2\n",
        "    ax1.legend(lines, labels, loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_lyapunov_noise(lyap_jac_clean, lyap_jac_noise, labels, ylabel=r'Lyapunov exponent $\\lambda$ [s⁻¹]'):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        plt.plot(lyap_jac_clean[i], marker='o', label=f'{labels[i]} (clean)')\n",
        "        plt.plot(lyap_jac_noise[i], marker='x', linestyle='--', label=f'{labels[i]} (noise)')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title('HSLE: clean vs noisy inputs')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def scatter_plot_predictions(model, X_test, y_test, title_suffix='test set'):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    θ_true = y_test[:, 0]\n",
        "    ω_true = y_test[:, 1]\n",
        "    θ_pred = y_pred[:, 0]\n",
        "    ω_pred = y_pred[:, 1]\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Theta\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(θ_true, θ_pred, alpha=0.5, label='Predictions')\n",
        "    plt.plot([θ_true.min(), θ_true.max()], [θ_true.min(), θ_true.max()], 'r--', label='Ideal prediction')\n",
        "    plt.xlabel('Normalized true θ [rad]')\n",
        "    plt.ylabel('Normalized predicted θ [rad]')\n",
        "    plt.title(f'Normalized angle prediction (test set)')\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    # Omega\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(ω_true, ω_pred, alpha=0.5, label='Predictions')\n",
        "    plt.plot([ω_true.min(), ω_true.max()], [ω_true.min(), ω_true.max()], 'r--', label='Ideal prediction')\n",
        "    plt.xlabel('Normalized true ω [rad/s]')\n",
        "    plt.ylabel('Normalized predicted ω [rad/s]')\n",
        "    plt.title(f'Normalized angular velocity prediction (test set)')\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_solution_curve():\n",
        "    \"\"\"\n",
        "    Inspection of solution curves for the simple or double pendulum.\n",
        "    \"\"\"\n",
        "    t = np.linspace(0, 10, 1000)\n",
        "\n",
        "    if system_type == 'simple':\n",
        "        θ1, ω1, θ2, ω2 = solve_pendulum(t, [i/10 for i in y0])\n",
        "    else:\n",
        "        θ1, ω1, θ2, ω2 = solve_pendulum(t, y0)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # θ₁ and θ₂\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(t, θ1, label=r'$\\theta_1(t)$')\n",
        "\n",
        "    if system_type == 'double':\n",
        "        plt.plot(t, θ2, label=r'$\\theta_2(t)$', linestyle='--')\n",
        "\n",
        "    plt.ylabel('Angle [rad]')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # ω₁ and ω₂\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(t, ω1, label=r'$\\omega_1(t)$')\n",
        "\n",
        "    if system_type == 'double':\n",
        "        plt.plot(t, ω2, label=r'$\\omega_2(t)$', linestyle='--')\n",
        "\n",
        "    plt.xlabel('Time [s]')\n",
        "    plt.ylabel('Angular velocity [rad/s]')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.suptitle(f'Solutions curve for the {system_type} pendulum')\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_predicted_vs_true_lyaps_dual(initials_lyap, lyap_cb, labels=None):\n",
        "    \"\"\"\n",
        "    Scatter plot: predicted Lyapunov exponents vs true ones for both prediction-based and Jacobian-based methods.\n",
        "\n",
        "    Parameters:\n",
        "    - initials_lyap: list of true Lyapunov exponents (ground truth)\n",
        "    - lyap_cb: LyapunovCallback instance with prediction history\n",
        "    - labels: optional list of labels for annotation\n",
        "    \"\"\"\n",
        "    # Get final values from last epoch\n",
        "    lyap_pred = [lyaps[-1] for lyaps in lyap_cb.lyap_list_all]\n",
        "    lyap_jac = [lyaps[-1] for lyaps in lyap_cb.lyap_list_jac_all]\n",
        "\n",
        "    plt.figure(figsize=(6.5, 6))\n",
        "    plt.scatter(initials_lyap, lyap_pred, color='blue', marker='o', label='PLE')\n",
        "    plt.scatter(initials_lyap, lyap_jac, color='green', marker='x', label='HSLE')\n",
        "\n",
        "    if labels:\n",
        "        for i, label in enumerate(labels):\n",
        "            plt.annotate(label, (initials_lyap[i], lyap_pred[i]), textcoords=\"offset points\", xytext=(5, 5), ha='left', fontsize=9)\n",
        "            plt.annotate(label, (initials_lyap[i], lyap_jac[i]), textcoords=\"offset points\", xytext=(5, -10), ha='left', fontsize=9)\n",
        "\n",
        "    plt.xlabel(\"Physical systems' Lyapunov exponent [s⁻¹]\")\n",
        "    plt.ylabel('Lyapunov exponent [s⁻¹]')\n",
        "    plt.title(\"Lyapunov exponent vs physical systems' Lyapunov exponent\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ===================================== RUN CODE ==========================================\n",
        "\n",
        "# Parameters\n",
        "system_type = 'double' # System: 'simple', 'simple chaotic' or 'double'\n",
        "angle_choice = 1 # Angle to use: 1 (θ₁) or 2 (θ₂)\n",
        "\n",
        "\n",
        "# -------- Hyperparameters\n",
        "N = 1000 # number of training sequences\n",
        "window_size = 15\n",
        "mean = 0\n",
        "sigma = 1\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "# ================ only for estimating analytical Lyapunov exponent ================\n",
        "end = 50\n",
        "T_max = 50\n",
        "num_points = 5000\n",
        "t = np.linspace(0, T_max, num_points)\n",
        "#============================================================================\n",
        "\n",
        "i0 = 0 # must be 0 for exact lyapunov prediction to make sense. Starting point after the trainig points.\n",
        "k = 10\n",
        "number_exponents = 20 # number of Lyapunov exponents to compute\n",
        "hidden_dim = 32   # size of hidden state\n",
        "\n",
        "\n",
        "# ============================= INSPECTION =====================================\n",
        "\n",
        "#plot_solution_curve()\n",
        "# plot_lyapunov_time(t, lp_network, 'Network FTLE') # estiamte LE of the physical system as a function of time\n",
        "\n",
        "# ================================== TRAINING ==================================\n",
        "\n",
        "\n",
        "X_train_all, y_train_all, X_pert_all = [], [], []\n",
        "for init in initials:\n",
        "    X, y, X_pert, *_ = get_data_full_state(init)\n",
        "    X_train_all.append(X)\n",
        "    y_train_all.append(y)\n",
        "    X_pert_all.append(X_pert)\n",
        "\n",
        "X_train = np.concatenate(X_train_all)\n",
        "y_train = np.concatenate(y_train_all)\n",
        "X_pert_train = np.concatenate(X_pert_all)\n",
        "\n",
        "\n",
        "model = define_model()\n",
        "rnn_layer = model.layers[1]\n",
        "tanh_prim = lambda x: 1 - np.tanh(x)**2\n",
        "#lyap_cb = LyapunovCallback(test_sets, k, number_exponents)\n",
        "\n",
        "results = train_model()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_lyaps(test_set, model, k, no_exps):\n",
        "    (X_clean_seq, _, X_pert_seq) = test_set\n",
        "    lyaps = []\n",
        "    lyaps_jac = []\n",
        "\n",
        "    for i in range(len(X_clean_seq)):\n",
        "        x_clean = X_clean_seq[i]\n",
        "        x_pert = X_pert_seq[i]\n",
        "\n",
        "        x_noise = x_pert - x_clean # noise in phase space\n",
        "        input_noise_norm = np.linalg.norm(x_noise) / x_clean.shape[0]\n",
        "\n",
        "        θ_clean_pred, ω_clean_pred = predict_k_steps(model, x_clean, k)\n",
        "        θ_pert_pred, ω_pert_pred = predict_k_steps(model, x_pert, k)\n",
        "\n",
        "        output_noise = np.stack([θ_pert_pred - θ_clean_pred, ω_pert_pred - ω_clean_pred], axis=1)\n",
        "        output_noise_norm = np.linalg.norm(output_noise) / len(output_noise)\n",
        "\n",
        "        lyap = lp_numeric(k, input_noise_norm, output_noise_norm)\n",
        "        lyaps.append(lyap)\n",
        "\n",
        "\n",
        "    # Jacobian method on a few\n",
        "    H0 = tf.zeros((len(X_clean_seq),hidden_dim)) # initial hidden state, shape (hidden_dim, 1)\n",
        "\n",
        "\n",
        "\n",
        "    lyap_jac = max(calc_LEs(X_clean_seq, H0, rnn_layer, tanh_prim, no_exps)[0])\n",
        "    # lyap_jac_noise = max(calc_LEs(x_clean_batch_noise, H0, rnn_layer, tanh_prim, self.number_exponents)[0])\n",
        "\n",
        "    return np.mean(lyaps), np.mean(lyaps_jac)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "Yczl4CtMl53m",
        "outputId": "93f141d2-4b05-41f6-b81a-b024a4ebb1c9"
      },
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer \"simple_rnn_cell\" \"                 f\"(type SimpleRNNCell).\n\n{{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:GPU:0}} In[0] and In[1] has different ndims: [2] vs. [2,32] [Op:MatMul]\n\nCall arguments received by layer \"simple_rnn_cell\" \"                 f\"(type SimpleRNNCell):\n  • inputs=tf.Tensor(shape=(2,), dtype=float32)\n  • states=tf.Tensor(shape=(32,), dtype=float32)\n  • training=None",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[35], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m test_sets, X_test, y_test, X_pert_test \u001b[38;5;241m=\u001b[39m get_test_sets(initials) \u001b[38;5;66;03m# use last one for testing --> scatter plot\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, test_set \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_sets):\n\u001b[1;32m----> 8\u001b[0m     PLE, HSLE\u001b[38;5;241m=\u001b[39m\u001b[43mcalc_lyaps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_exponents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlyapunov_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     10\u001b[0m         writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(file)\n",
            "Cell \u001b[1;32mIn[34], line 28\u001b[0m, in \u001b[0;36mcalc_lyaps\u001b[1;34m(test_set, model, k, no_exps)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Jacobian method on a few\u001b[39;00m\n\u001b[0;32m     24\u001b[0m H0 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(X_clean_seq),hidden_dim)) \u001b[38;5;66;03m# initial hidden state, shape (hidden_dim, 1)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m lyap_jac \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[43mcalc_LEs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_clean_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtanh_prim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_exps\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# lyap_jac_noise = max(calc_LEs(x_clean_batch_noise, H0, rnn_layer, tanh_prim, self.number_exponents)[0])\u001b[39;00m\n\u001b[0;32m     31\u001b[0m lyaps_jac\u001b[38;5;241m.\u001b[39mappend(lyap_jac)\n",
            "Cell \u001b[1;32mIn[5], line 202\u001b[0m, in \u001b[0;36mcalc_LEs\u001b[1;34m(x_batches, h0, RNNlayer, activation_function_prim, k_LE)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m#Get next state ht+1 by taking a reccurent step\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     xt\u001b[38;5;241m=\u001b[39mx[t]\n\u001b[1;32m--> 202\u001b[0m     _, ht \u001b[38;5;241m=\u001b[39m \u001b[43mRNNcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mht\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m#Get jacobian J\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     Wxh, Whh, b \u001b[38;5;241m=\u001b[39m rnn_layer\u001b[38;5;241m.\u001b[39mget_weights()\n",
            "File \u001b[1;32mc:\\Users\\Abbe\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\Abbe\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py:2455\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   2453\u001b[0m     out \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39msparse_dense_matmul(x, y)\n\u001b[0;32m   2454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2455\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"simple_rnn_cell\" \"                 f\"(type SimpleRNNCell).\n\n{{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:GPU:0}} In[0] and In[1] has different ndims: [2] vs. [2,32] [Op:MatMul]\n\nCall arguments received by layer \"simple_rnn_cell\" \"                 f\"(type SimpleRNNCell):\n  • inputs=tf.Tensor(shape=(2,), dtype=float32)\n  • states=tf.Tensor(shape=(32,), dtype=float32)\n  • training=None"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "initials = [[np.pi/2, 1, np.pi/2, 1], [np.pi/4, -1, np.pi/4, -1], [-np.pi/2, 0, np.pi/2, 0], [0.1, 2.0, 3*np.pi/4, -1.5], [np.pi, 0.5, -np.pi/2, -0.5], [-np.pi, -2.0, np.pi/3, 1.0]]\n",
        "initials_lyaps = [1.40176, 1.59057, 2.28863, 2.7653, 2.8809, 2.94079]\n",
        "test_sets, X_test, y_test, X_pert_test = get_test_sets(initials) # use last one for testing --> scatter plot\n",
        "for i, test_set in enumerate(test_sets):\n",
        "    PLE, HSLE=calc_lyaps(test_set, model, k, number_exponents)\n",
        "    with open('lyapunov_results.csv', mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([initials_lyaps[i], PLE, HSLE])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOkq02ygSmTfjRXhOTXjWPq",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
